### dl 
dl is a small library I made to understand how neural networks works and gradient descent optimizers work. I have benchmarked SGD, Adam and RMSprop, used dl for simple problems like classifying MNIST digits and modelling logic gates. Adding classes for CNNs and LSTMs, tweaking optimizer class for arbitrary parameters and making the API a bit more PyTorch-like (like https://github.com/karpathy/micrograd) are a few ways to make this library better. But, it can be used for writing custom training loops (using the backward method in the Model class) for algorithms like REINFORCE or DQL. 
