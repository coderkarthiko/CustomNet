### dl 
dl is a small library I made to understand how neural networks work. I have implemented GD optimizers like SGD with momentum, Adam and RMSprop. Earlier, I didn't know about weight initialization techniques. Everytime I initialized a network, I manually specified the uniform distribution bounds. Now, it uses XAVIER initialization and it performs MUCH better. I tweaked the bounds for biases that use XAVIER slightly (i.e. they are intialized with positive values) to prevent dying ReLUs. 
   Activation functions are sigmoid, tanh, ReLU and softmax. If final layer is softmax or sigmoid and the loss function is cross entropy, there is no need to compute the cross entropy loss - we can directly compute the derivative of loss function with respect to inputs of neurons in final layer. The derivative dJ/da^L = y - z^L where z^L -> output layer and y -> labels. 
   Adam performed considerable well on MNIST compared to SGD with momentum or RMSprop. It IS a toy dataset, but 96% accuracy (see MNIST.ipynb) after 3 minutes of training (4 epochs using ReLUs, softmax output and cross-entropy loss) is vastly better than 96% accuracy after 3 hours of training (100 epochs using sigmoid, MSE loss and no XAVIER initialization). The optimizer class also makes it easier to implement custom training loops (useful for some basic RL algorithms like REINFORCE). 
   It does okay on real word data as well! I took photos of some handwritten digits using my phone and it predicted almost all of them correctly! For datasets like FASHIONMNIST some HP tuning is required as there is a lot of correlation between items of clothing. I may implement CNNs later. 
