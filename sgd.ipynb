{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import customnet as cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [np.array([20, 8, 3, 7, 5, 15, 2, 12, 10, 4])]\n",
    "y = [np.array([12, 4, 5, 6, 14, 1, 15, 7, 15, 15])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.47it/s]\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 1e-1\n",
    "BETA = 0.667\n",
    "EPOCHS = 1\n",
    "\n",
    "model = cn.Model([10, 5, 10], ['sigmoid', 'relu'])\n",
    "\n",
    "model.SGD(x, y, batch_size=1, loss_fn='mse', alpha=ALPHA, beta=BETA, epochs=EPOCHS)\n",
    "out = model.forward(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label -  [12  4  5  6 14  1 15  7 15 15] \n",
      "prediction -  [12.00171212  4.0000044   5.00046427  6.0006253  14.00225913  0.99949645\n",
      " 15.00227631  7.00110236 15.00251774 15.00214275] \n",
      "loss -  2.622232410463543e-05\n"
     ]
    }
   ],
   "source": [
    "print('label - ', y[0], '\\nprediction - ', out, '\\nloss - ', np.sum((y[0] - out) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training for just a single epoch, SGD with momentum results in prediction being accurate upto 2 decimal places and incurs an MSE loss of just 0.000026. The magic is fully explained here - https://distill.pub/2017/momentum/. The basic gist? SGD follows the update rule $w:=w-\\frac{\\eta}{m}\\nabla_w f(x, w, b)$ and $b:=b-\\frac{\\eta}{m}\\nabla_b f(x, w, b)$. SGD with momentum is the same except we replace $\\nabla_w f(x, w, b)$ and $\\nabla_b f(x, w, b)$ with $v_w$ and $v_b$ and they both follow a seperate update rule - $v_w:=\\beta v_w+\\nabla_w f(x, w, b)$ and $v_b:=\\beta v_b+\\nabla_b f(x, w, b)$. In all the websites I looked, no info was given about the initial values of $v_w$ and $v_b$ so I initialized them as zero vectors of the same dimension as $w$ and $b$ respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
